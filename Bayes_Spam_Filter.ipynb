{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Spam Filter with Naive Bayes\n",
    "\n",
    "In this project, we're going to build a spam filter for use on text messages using Naive Bayes. To classify messages as spam or non-spam, the Naive Bayes algorithm follows these steps:\n",
    "\n",
    "- Learns how humans classify messages.\n",
    "- Uses that human knowledge to estimate probabilities for new messages — probabilities for spam and non-spam.\n",
    "- Classifies a new message based on these probability values — if the probability for spam is greater, then it classifies the message as spam. Otherwise, it classifies it as non-spam (if the two probability values are equal, then we may need a human to classify the message).\n",
    "\n",
    "\n",
    "## Dataset\n",
    "To teach the algorithm how to classify messages, we'll use a dataset of 5,572 SMS messages that have been classified by humans. \n",
    "\n",
    "The dataset was put together by Tiago A. Almeida and José María Gómez Hidalgo, and it can be downloaded from the [The UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/sms+spam+collection). \n",
    "\n",
    "The data is unlabelled, so we'll name the two columns:\n",
    "- ```Label```: whether the message is spam (```spam```) or not spam (```ham```).\n",
    "- ```SMS```: the actual text message content\n",
    "\n",
    "## Goal\n",
    "Build a multinomial Naive Bayes algorithm to classify SMS messages as spam or non-spam. We want to achieve at least 80% accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sms_messages = pd.read_csv('SMSSpamCollection', sep='\\t',\n",
    "                          header=None, names=['Label','SMS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>SMS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Label                                                SMS\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms_messages.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5572, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms_messages.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     4825\n",
       "spam     747\n",
       "Name: Label, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms_messages['Label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     86.593683\n",
       "spam    13.406317\n",
       "Name: Label, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms_messages['Label'].value_counts(normalize=True)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Testing Set\n",
    "\n",
    "When loading the dataset, we see that about 86% of the SMS messages are not spam while the remaining 14% are spam. Let's split the data into a training set and a testing set so that at the end we can evaluate the success of our spam filter. We'll use 80% for training and 20% for testing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# randomize the entire dataset\n",
    "random_sms = sms_messages.sample(frac=1,random_state=42)\n",
    "\n",
    "# Calculate index for split\n",
    "training_test_index = round(len(random_sms) * 0.8)\n",
    "\n",
    "# split into training and testing\n",
    "training = random_sms[:training_test_index].reset_index(drop=True)\n",
    "testing = random_sms[training_test_index:].reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     86.698071\n",
       "spam    13.301929\n",
       "Name: Label, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training['Label'].value_counts(normalize=True)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     86.175943\n",
       "spam    13.824057\n",
       "Name: Label, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing['Label'].value_counts(normalize=True)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that we have approximately the same percentage of spam and non-spam messages in our training and our testing datasets as in the full dataset. This is exactly what we want."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Letter Case and Punctuation\n",
    "\n",
    "Let's begin the data cleaning process by removing the punctuation and bringing all the words to lower case. In the end, the algorithm will look calculate probabilities word-by-word, so we want to standardize the formatting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training['SMS'] = training['SMS'].str.replace('\\W', ' ')\n",
    "training['SMS'] = training['SMS'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    squeeeeeze   this is christmas hug   if u lik ...\n",
       "1    and also i ve sorta blown him off a couple tim...\n",
       "2    mmm thats better now i got a roast down me  i ...\n",
       "3        mm have some kanji dont eat anything heavy ok\n",
       "4    so there s a ring that comes with the guys cos...\n",
       "Name: SMS, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training['SMS'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testing['SMS'] = testing['SMS'].str.replace('\\W', ' ')\n",
    "testing['SMS'] = testing['SMS'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    was playng 9 doors game and gt racing on phone...\n",
       "1           i dont thnk its a wrong calling between us\n",
       "2                          all e best 4 ur exam later \n",
       "3     hey what how about your project  started aha da \n",
       "4    dunno  my dad said he coming home 2 bring us o...\n",
       "Name: SMS, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing['SMS'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Creating the Vocabulary\n",
    "\n",
    "In order to run the algorithm, we need to create a vocabulary of every word that is included in each message. We'll also want to know if words appear multiple times in the same message. To get this information in an easy to use way, let's add a column for every unique word in the messages. The row values will then reflect the number of times that word appears in that message. For example, if the word *computer* does not appear in a message, the value will be 0. But if the word *computer* appears twice in a different message, the value will be 2. \n",
    "\n",
    "To do this, let's first split each message into a list, then create a list of all unique words that appear. We'll see that we end up with 7816 unique words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# transform each message from the SMS column into a list\n",
    "\n",
    "training['SMS'] = training['SMS'].str.split()\n",
    "\n",
    "vocabulary = []\n",
    "\n",
    "for message in training['SMS']:\n",
    "    for word in message:\n",
    "        vocabulary.append(word)\n",
    "        \n",
    "vocabulary = set(vocabulary)\n",
    "\n",
    "vocabulary = list(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7816\n"
     ]
    }
   ],
   "source": [
    "print(len(vocabulary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Final Training Set\n",
    "\n",
    "Now that we have our vocabulary, we need to use it to populate our dataset for analysis. To do this, we will first build a dictionary that we will then convert to a new DataFrame. The dictionary will include each unique word as keys, and the values will be a list of frequencies by message. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# initialize the dictionary\n",
    "word_counts_per_sms = {unique_word: [0] * len(training['SMS'])\n",
    "                      for unique_word in vocabulary}\n",
    "\n",
    "# loop over training['SMS'] using the enumerate function\n",
    "\n",
    "for index, sms in enumerate(training['SMS']):\n",
    "    for word in sms:\n",
    "        word_counts_per_sms[word][index] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_counts_df = pd.DataFrame(word_counts_per_sms)\n",
    "\n",
    "analysis_df = pd.concat([training,word_counts_df],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>SMS</th>\n",
       "      <th>0</th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>008704050406</th>\n",
       "      <th>0121</th>\n",
       "      <th>01223585236</th>\n",
       "      <th>01223585334</th>\n",
       "      <th>0125698789</th>\n",
       "      <th>...</th>\n",
       "      <th>zoe</th>\n",
       "      <th>zogtorius</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zouk</th>\n",
       "      <th>zyada</th>\n",
       "      <th>é</th>\n",
       "      <th>ú1</th>\n",
       "      <th>ü</th>\n",
       "      <th>〨ud</th>\n",
       "      <th>鈥</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>[squeeeeeze, this, is, christmas, hug, if, u, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>[and, also, i, ve, sorta, blown, him, off, a, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>[mmm, thats, better, now, i, got, a, roast, do...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>[mm, have, some, kanji, dont, eat, anything, h...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>[so, there, s, a, ring, that, comes, with, the...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 7818 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Label                                                SMS  0  00  000  \\\n",
       "0   ham  [squeeeeeze, this, is, christmas, hug, if, u, ...  0   0    0   \n",
       "1   ham  [and, also, i, ve, sorta, blown, him, off, a, ...  0   0    0   \n",
       "2   ham  [mmm, thats, better, now, i, got, a, roast, do...  0   0    0   \n",
       "3   ham  [mm, have, some, kanji, dont, eat, anything, h...  0   0    0   \n",
       "4   ham  [so, there, s, a, ring, that, comes, with, the...  0   0    0   \n",
       "\n",
       "   008704050406  0121  01223585236  01223585334  0125698789 ...  zoe  \\\n",
       "0             0     0            0            0           0 ...    0   \n",
       "1             0     0            0            0           0 ...    0   \n",
       "2             0     0            0            0           0 ...    0   \n",
       "3             0     0            0            0           0 ...    0   \n",
       "4             0     0            0            0           0 ...    0   \n",
       "\n",
       "   zogtorius  zoom  zouk  zyada  é  ú1  ü  〨ud  鈥  \n",
       "0          0     0     0      0  0   0  0    0  0  \n",
       "1          0     0     0      0  0   0  0    0  0  \n",
       "2          0     0     0      0  0   0  0    0  0  \n",
       "3          0     0     0      0  0   0  0    0  0  \n",
       "4          0     0     0      0  0   0  0    0  0  \n",
       "\n",
       "[5 rows x 7818 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analysis_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Constants\n",
    "\n",
    "We now have our training data that we are ready to work with. As a reminder, we will use the following two equations to calculate the probabilities of the messages being spam or not-spam. \n",
    "\n",
    "$P(Spam|w_1, w_2, ..., w_n) \\propto P(Spam) \\cdot \\prod_{i=1}^{n} P(w_i | Spam)$\n",
    "\n",
    "$P(Ham|w_1, w_2, ..., w_n) \\propto P(Ham) \\cdot \\prod_{i=1}^{n} P(w_i | Ham)$\n",
    "\n",
    "And to calculate $P(w_i | Spam)$ and $P(w_i | Ham)$ we will use\n",
    "\n",
    "$ P(w_i | Spam) = \\frac{N_{w_i | Spam} + \\alpha}{N_{Spam} + \\alpha \\cdot N_{vocabulary}} $\n",
    "\n",
    "$ P(w_i | Ham) = \\frac{N_{w_i | Ham} + \\alpha}{N_{Ham} + \\alpha \\cdot N_{vocabulary}} $\n",
    "\n",
    "Some of these values will be constant each message. Let's calculate these constants first:\n",
    "\n",
    "- $P(Spam)$\n",
    "- $P(Ham)$\n",
    "- $N_{spam}$ - number of words in all spam messages\n",
    "- $N_{ham}$ - number of words in all ham messages\n",
    "- $N_{vocabulary}$\n",
    "\n",
    "For now, we will let alpha = 1 for Laplace smoothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Isolating spam and ham messages first\n",
    "spam_messages = analysis_df[analysis_df['Label'] == 'spam']\n",
    "ham_messages = analysis_df[analysis_df['Label'] == 'ham']\n",
    "\n",
    "# P(Spam) and P(Ham)\n",
    "p_spam = len(spam_messages) / len(analysis_df)\n",
    "p_ham = len(ham_messages) / len(analysis_df)\n",
    "\n",
    "# N_Spam\n",
    "n_words_per_spam_message = spam_messages['SMS'].apply(len)\n",
    "n_spam = n_words_per_spam_message.sum()\n",
    "\n",
    "# N_Ham\n",
    "n_words_per_ham_message = ham_messages['SMS'].apply(len)\n",
    "n_ham = n_words_per_ham_message.sum()\n",
    "\n",
    "# N_Vocabulary\n",
    "n_vocabulary = len(vocabulary)\n",
    "\n",
    "# Laplace smoothing\n",
    "alpha = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Calculating Parameters\n",
    "\n",
    "Now, let's calculate $P(w_i|Spam)$ and $P(w_i|Ham)$ for each word. These values will be constant for the whole training set, so if we calculate these parameters now, we will save computational time in the future. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p_spam_dict = {unique_word: 0\n",
    "                      for unique_word in vocabulary}\n",
    "p_ham_dict = {unique_word: 0\n",
    "                      for unique_word in vocabulary}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# we isolated the spam and ham messages above as\n",
    "# spam_messages and ham_messages\n",
    "\n",
    "# calculate p_spam\n",
    "\n",
    "for word in vocabulary:\n",
    "    \n",
    "    # calculate number of times the word appears\n",
    "    # in spam and ham messages\n",
    "    n_word_given_spam = spam_messages[word].sum()\n",
    "    n_word_given_ham = ham_messages[word].sum()\n",
    "    \n",
    "    \n",
    "    p_word_given_spam = (n_word_given_spam + alpha) / (n_spam + alpha*n_vocabulary)\n",
    "    p_word_given_ham = (n_word_given_ham + alpha) / (n_ham + alpha*n_vocabulary)\n",
    "    \n",
    "    p_spam_dict[word] = p_word_given_spam\n",
    "    p_ham_dict[word] = p_word_given_ham"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifying a New Message\n",
    "\n",
    "Now that we have calculated all the constants and parameters, we can create the spam filter. This filter will:\n",
    "\n",
    "- take in as input a new message\n",
    "- Calculates P(Spam|w1, w2, ..., wn) and P(Ham|w1, w2, ..., wn)\n",
    "- Compares the values of P(Spam|w1, w2, ..., wn) and P(Ham|w1, w2, ..., wn), and:\n",
    "    - If P(Ham|w1, w2, ..., wn) > P(Spam|w1, w2, ..., wn), then the message is classified as ham.\n",
    "    - If P(Ham|w1, w2, ..., wn) < P(Spam|w1, w2, ..., wn), then the message is classified as spam.\n",
    "    - If P(Ham|w1, w2, ..., wn) = P(Spam|w1, w2, ..., wn), then the algorithm may request human help.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def classify(message):\n",
    "\n",
    "    message = re.sub('\\W', ' ', message)\n",
    "    message = message.lower()\n",
    "    message = message.split()\n",
    "\n",
    "    \n",
    "    p_spam_given_message = p_spam\n",
    "    p_ham_given_message = p_ham\n",
    "    \n",
    "    for word in message:\n",
    "        if word in p_spam_dict:\n",
    "            p_spam_given_message *= p_spam_dict[word]\n",
    "            \n",
    "        if word in p_ham_dict:\n",
    "            p_ham_given_message *= p_ham_dict[word]\n",
    "\n",
    "    print('P(Spam|message):', p_spam_given_message)\n",
    "    print('P(Ham|message):', p_ham_given_message)\n",
    "\n",
    "    if p_ham_given_message > p_spam_given_message:\n",
    "        print('Label: Ham')\n",
    "    elif p_ham_given_message < p_spam_given_message:\n",
    "        print('Label: Spam')\n",
    "    else:\n",
    "        print('Equal proabilities, have a human classify this!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(Spam|message): 5.195728000170284e-16\n",
      "P(Ham|message): 2.248098033852803e-17\n",
      "Label: Spam\n"
     ]
    }
   ],
   "source": [
    "classify('secret money prince code.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(Spam|message): 4.74693224259436e-25\n",
      "P(Ham|message): 2.7878169823581606e-21\n",
      "Label: Ham\n"
     ]
    }
   ],
   "source": [
    "classify('Sounds good, Tom, then see u there.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(Spam|message): 1.5223001843661562e-25\n",
      "P(Ham|message): 1.2176099861344542e-27\n",
      "Label: Spam\n"
     ]
    }
   ],
   "source": [
    "classify('WINNER!! This is the secret code to unlock the money: C3421.')\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring the Spam Filter's Accuracy\n",
    "\n",
    "Now that we have created our spam filter, let's test it on our testing set of 1,114 messages. We will modify the ```classify``` function we wrote last time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classify_test_set(message):\n",
    "\n",
    "    message = re.sub('\\W', ' ', message)\n",
    "    message = message.lower()\n",
    "    message = message.split()\n",
    "\n",
    "    p_spam_given_message = p_spam\n",
    "    p_ham_given_message = p_ham\n",
    "\n",
    "    for word in message:\n",
    "        if word in p_spam_dict:\n",
    "            p_spam_given_message *= p_spam_dict[word]\n",
    "\n",
    "        if word in p_ham_dict:\n",
    "            p_ham_given_message *= p_spam_dict[word]\n",
    "\n",
    "    if p_ham_given_message > p_spam_given_message:\n",
    "        return 'ham'\n",
    "    elif p_spam_given_message > p_ham_given_message:\n",
    "        return 'spam'\n",
    "    else:\n",
    "        return 'needs human classification'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "testing['predicted'] = testing['SMS'].apply(classify_test_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct: 956\n",
      "Incorrect: 158\n",
      "Accuracy: 0.8581687612208259\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = len(testing)\n",
    "\n",
    "for row in testing.iterrows():\n",
    "    row = row[1]\n",
    "    if row['Label'] == row['predicted']:\n",
    "        correct += 1\n",
    "        \n",
    "print('Correct:', correct)\n",
    "print('Incorrect:', total - correct)\n",
    "print('Accuracy:', correct/total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
